- Ollama is a tool that lets you run Large Language Models (LLMs) locally on your own computerâ€”without using cloud services, without sending data anywhere, and without API costs.
- You can call Ollama from:
  - Terminal
  - Python
  - Node.js
  - REST API
 
- ollama list : List all installed models
- ollama pull llama3 : download a model
- ollama run llama3  : run a model
- (Press Ctrl + C) : stop running model
- ollama show llama3 : Show details of a model
- ollama rm llama3  : delete a model
- Ollama runs a local server at: http://localhost:11434
- Chat completion API http://localhost:11434/api/chat
- Generate API http://localhost:11434/api/generate
- ollama logs : Show Ollama server logs
- ollama serve : Start Ollama service manually
  
